import nltk
import numpy, pprint, pickle, random, string, pandas, re,sys, os, codecs
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from sklearn import svm, linear_model, naive_bayes
from nltk.stem.wordnet import WordNetLemmatizer

d_in = pandas.read_csv("train_in.csv")
d_out = pandas.read_csv("train_out.csv")
merged = d_out.merge(d_in, on="id", how="outer").fillna("")
merged.to_csv("merged.csv",index=False)

with open('merged.csv','rb') as data_source:
    data_T = data_source.readlines()
    #print len(data_T)
    c = 0
    for row in data_T:
        category = row.split(',')[0]
        paper_id = row.split(',')[1]
        abstract = row.split(',')[2:]
        string = " ".join(str(elm) for elm in abstract)
        lower = string.lower()
        tokens = word_tokenize(lower)
        #print paper_id
        #print category
        #print abstract
        c+= 1
        if c == 3:
            break

    X = numpy.array(data_T)[:4]
    #print X.shape
    #print X
    for row in X:
        id_X = row.split(',')[0]
        cat_X = row.split(',')[1]
        abs_X = row.split(',')[2:]
        string_X = " ".join(str(elm) for elm in abs_X)
        lower_X = string_X.lower()
        tokens_X = word_tokenize(lower_X)
        words = ','.join(tokens_X)
        print id_X
        print cat_X
        print words
        #tokens_X_array = numpy.array(tokens_X)
       # X_lem = wnl.lemmatize(tokens_X_array[1,1])
        from nltk.stem.wordnet import WordNetLemmatizer
        for row in words:
            lmtzr = WordNetLemmatizer()
            lem_X = lmtzr.lemmatize(words[3], 'v')
            print id_X
            print lem_X

        #X = numpy.array(tokens_X)[:3]
        #print X.shape
        #print X
        #print len(X)
        i=0
        for row in words:
            lem_X = lmtzr.lemmatize(words[i],'v')


    import collections
    count = collections.Counter(tokens_X)
    print(count.values())
    print(count.keys())

    from sklearn.feature_extraction.text import CountVectorizer
    count_vect = CountVectorizer()
    out = count_vect.fit_transform(X)
    #count_vect.fit(my_dataset) # my_Dataset = ["document 1: blablabla...", "document 2: ...", ]
    #feature_of_one_example = count_vect.transform(one_example) -> [0,0,0,0,...,1,3,0...]


